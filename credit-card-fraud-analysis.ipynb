{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nnp.random.seed(42) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/creditcardfraud/creditcard.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"This data frame has {} rows and {} columns\".format(df.shape[0], df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analyze known values"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option(\"precision\", 5)\ndf.loc[:, ['Time', 'Amount']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.title(\"Distribution of Time Feature\")\nsns.histplot(df.Time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.title('Distribution of Monetary Value Feature')\nsns.distplot(df.Amount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = df.Class.value_counts()\nprint(counts)\ngenuine = counts[0]\nfraud = counts[1]\nperc_fraud = fraud / (fraud + genuine) * 100\nperc_genuine = 100 - perc_fraud\nprint(\"Fraudulent transactions: ({:.3f}%), Genuine transactions: ({:.3f}%)\".format(perc_fraud, perc_genuine))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\nplt.figure(figsize=(12,10))\nheat = sns.heatmap(data=corr)\nplt.title(\"Heatmap of correlation\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scaling Amount and Time"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, RobustScaler\nfrom itertools import chain\n\n# robust scaler is better for outliers\nrob_scaler = RobustScaler() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_amount = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\nscaled_time = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\ndf.insert(0, 'scaled_time', scaled_time)\ndf.insert(1, 'scaled_amount', scaled_amount)\ndf.drop(['Time', 'Amount'], axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing training and testing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nprint(\"Fraudulent transactions: ({:.3f}%), Genuine transactions: ({:.3f}%)\".format(perc_fraud, perc_genuine))\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nskf = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in skf.split(X,y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n    \nprint('-' * 100)\nprint('Label Distributions: ')\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# Double check after splitting\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_uniqe_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint(train_counts_label)\nprint(\"Y Train\", train_counts_label/ len(original_ytrain))\nprint(\"Y Test\", test_counts_label/ len(original_ytest))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Undersampling - Prepare data to fit into models"},{"metadata":{"trusted":true},"cell_type":"code","source":"#shuffle data\ndf = df.sample(frac=1)\n\n# we only have 492 fradulent transactions so we will pick 492 genuine ones too\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnew_df = pd.concat([fraud_df, non_fraud_df]).sample(frac=1, random_state=16)\nnew_df.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Distribution of the classes in undersampling\")\nprint(new_df['Class'].value_counts() / len(new_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = [\"#0101DF\", \"#DF0101\"]\nplt.title(\"Equally distributed classes\")\nsns.countplot('Class', data=new_df, palette=colors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"subsample_corr = new_df.corr()\nplt.figure(figsize=(12,10))\nsubsample_heat = sns.heatmap(data=subsample_corr)\nplt.title(\"Heatmap of sub sample correlation\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Negative correlation with Class: V3, V9, V10, V12, V14, V16, V17 -> the lower it is, the more likely it will be a fraud\n\n\nPositive correlation with Class: V4, V11 -> the higher it is, the more likely it will be a fraud\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = new_df.corr()\ncorr = corr[['Class']]\ncorr\ncorr[corr.Class < -0.5]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr[corr.Class > 0.5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Negative correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(nrows=2, ncols=4, figsize=(26,16))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V3\", data=new_df, palette=colors, ax=axes[0, 0])\naxes[0, 0].set_title('V3 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V9\", data=new_df, palette=colors, ax=axes[0, 1])\naxes[0, 1].set_title('V9 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V11\", data=new_df, palette=colors, ax=axes[0, 2])\naxes[0, 2].set_title('V11 vs Class Negative Correlation')\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=colors, ax=axes[0, 3])\naxes[0, 3].set_title('V12 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=colors, ax=axes[1, 0])\naxes[1, 0].set_title('V14 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V16\", data=new_df, palette=colors, ax=axes[1, 1])\naxes[1, 1].set_title('V16 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V17\", data=new_df, palette=colors, ax=axes[1, 2])\naxes[1, 2].set_title('V17 vs Class Negative Correlation')\n\nf.delaxes(axes[1, 3])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Possitive correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(ncols=2, figsize=(14, 8))\n\nsns.boxplot(x=\"Class\", y=\"V11\", data=new_df, palette=colors, ax=axes[0])\naxes[0].set_title(\"V11 vs Class Positive Correlation\")\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=new_df, palette=colors, ax=axes[1])\naxes[1].set_title('V4 vs Class Positive Correlation')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = new_df.quantile(0.25)\nQ3 = new_df.quantile(0.75)\nIQR = Q3 - Q1\n\nprint(\"Before:\", len(new_df))\n\nnew_df = new_df[~((new_df < (Q1 - 2.5 * IQR)) | (new_df > (Q3 + 2.5 * IQR))).any(axis=1)]\n\nprint(\"After\", len(new_df))\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dimension reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n#t-SNE\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\n\nprint(X_reduced_tsne)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# t-SNE scatter plot\nimport matplotlib.patches as mpatches\n\nf, ax = plt.subplots(figsize=(20,10))\n\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\nax.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax.set_title('t-SNE', fontsize=14)\n\nax.grid(True)\n\nax.legend(handles=[blue_patch, red_patch])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run Classification Algos"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nRunning algos:\n1. Decision trees with some pruning\n2. Neural networks: many layers and any activation function you see fit\n3. Boosting for the decision tree\n4. SVM: use least 2 kernels\n5. k-nearest neighbors -> use different k\n\"\"\"\nfrom sklearn.model_selection import train_test_split\n# under sampling before cross validating\n\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n# from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train = X_train.values\ny_train = y_train.values\nX_test = X_test.values\ny_test = y_test.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create neural network from Keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\ninput_size = X_train.shape[1]\ndef create_network(optimizer=\"adam\", activation=\"relu\"):\n    model = Sequential([\n        Dense(input_size, input_shape=(input_size, ), activation=activation),\n        Dense(32, activation=activation),\n        Dense(2, activation=activation)\n    ])\n    model.compile(optimizer=optimizer, loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\nnn_model = KerasClassifier(build_fn=create_network, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\n\nclassifiers = {\n    \"K Nearest Neighbor\": KNeighborsClassifier(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Support Vector Machine\": SVC(),\n    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n    \"Neural Network Classifier\": nn_model\n}\n\n# calculate training score based on X_train, y_train to compare performance of hyperparams\n# calculate cross_val_score on train data to compare performance of hyperparameter\nfor key, clf in classifiers.items():\n    training_score = cross_val_score(clf, X_train, y_train, cv=5)\n    print(\"Classifier: {}, training score: {}%\".format(key, training_score.mean() * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hyparameter tunning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# K nearest neighbor\nknn_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\nknn_grid = GridSearchCV(KNeighborsClassifier(), knn_params)\nknn_grid.fit(X_train, y_train)\nknn_best_params = knn_grid.best_params_\nprint(\"K Nearest Neighbor:\", knn_best_params)\n\n# Decision Tree\ndc_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,5,1)), \"min_samples_leaf\": list(range(5,7,1))}\ndc_grid = GridSearchCV(DecisionTreeClassifier(), dc_params)\ndc_grid.fit(X_train, y_train)\ndc_best_params = dc_grid.best_params_\nprint(\"Decision Tree:\", dc_best_params)\n\n\n# Support Vestor Machine\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\nsvc_grid = GridSearchCV(SVC(), svc_params)\nsvc_grid.fit(X_train, y_train)\nsvc_best_params = svc_grid.best_params_\n\nprint(\"Support Vector Machine:\",svc_best_params)\n\n# Gradient Boosting Classifier\ngb_params = {\"n_estimators\": [50, 150, 300], \"max_depth\": list(range(1,7,2)), \"min_samples_leaf\": [7,9,11,13]}\ngb_grid = GridSearchCV(GradientBoostingClassifier(), gb_params)\ngb_grid.fit(X_train, y_train)\ngb_best_params = gb_grid.best_params_\nprint(\"Gradient Boosting:\",gb_best_params)\n\n# Keras Neural Net Classifier\nnn_params = {\"nb_epoch\": [5, 10, 15], \"batch_size\": [5, 25, 50], \"optimizer\": [\"adam\", \"sgd\"], \"activation\": [\"tanh\", \"relu\", \"softmax\"]}\nnn_grid = GridSearchCV(nn_model, nn_params)\nnn_grid.fit(X_train, y_train)\nnn_best_params = nn_grid.best_params_\nprint(\"Neural network\", nn_best_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analysis"},{"metadata":{},"cell_type":"markdown","source":"Plot learning curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n\n# reference https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid(True)\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"#ff9124\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"#2492ff\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid(True)\n    axes[1].plot(train_sizes, fit_times_mean, 'o-', color=\"#ff9124\")\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1, color=\"#ff9124\")\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid(True)\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-', color=\"#ff9124\")\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1, color=\"#ff9124\")\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt\n\ntitle = \"Learning Curves (K Nearest Neighbor)\"\nplot_learning_curve(knn_grid.best_estimator_, title, X_train, y_train, axes=None, ylim=(0.7, 1.01), cv=None, n_jobs=None)\n\ntitle = \"Learning Curves (Decision Tree)\"\nplot_learning_curve(dc_grid.best_estimator_, title, X_train, y_train, axes=None, ylim=(0.7, 1.01), cv=None, n_jobs=None)\n\ntitle = \"Learning Curves (Support Vector Machine)\"\nplot_learning_curve(svc_grid.best_estimator_, title, X_train, y_train, axes=None, ylim=(0.7, 1.01), cv=None, n_jobs=None)\n\ntitle = \"Learning Curves (Gradient Boosting)\"\nplot_learning_curve(gb_grid.best_estimator_, title, X_train, y_train, axes=None, ylim=(0.7, 1.01), cv=None, n_jobs=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Learning Curves (Neural Network)\"\nplot_learning_curve(nn_grid.best_estimator_, title, X_train, y_train, axes=None, ylim=(0.4, 1.01), cv=None, n_jobs=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion matrix, recall, precision, fscore, support"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import precision_score\n\n# Now start training,the best estimator with orginal_Xtrain, original_ytrain\n# Then calculate precision\nbest_estimators = {\n    \"K Nearest Neighbor\": knn_grid.best_estimator_,\n    \"Decision Tree\": dc_grid.best_estimator_,\n    \"Gradient Boosting Classifier\": gb_grid.best_estimator_,\n}\n\n\nfor name, estimator in best_estimators.items():\n    estimator.fit(original_Xtrain, original_ytrain)\n    y_predict = estimator.predict(original_Xtest)\n    print(\"{} - Precision Score: {:.2f} %\".format(name, precision_score(original_ytest, y_predict) * 100))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_best = svc_grid.best_estimator_\nsvc_best.fit(original_Xtrain, original_ytrain)\ny_predict = svc_best.predict(original_Xtest)\nprint(\"Support Vector Machine - Precision Score: {:.2f} %\".format(precision_score(original_ytest, y_predict) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_best = create_network(optimizer=\"adam\", activation=\"tanh\")\nnn_best.fit(original_Xtrain, original_ytrain, batch_size=5, epochs=10)\ny_predict = nn_best.predict_classes(original_Xtest, batch_size=5)\nprint(y_predict)\nprint(\"Neural Network - Precision Score: {:.2f} %\".format(precision_score(original_ytest, y_predict) * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**REFERENCE**\n\nThe EPA is being referenced from https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}